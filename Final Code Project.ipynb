{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  \n",
    "import torch \n",
    "from sklearn.datasets import fetch_20newsgroups \n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "#import spacy\n",
    "import ast \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_news_train= fetch_20newsgroups(remove=(\"headers\",\"footers\"))\n",
    "twenty_news_test= fetch_20newsgroups(subset=\"test\",remove=(\"headers\",\"footers\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 0 Data Exploration\n",
    "print(twenty_news_train.DESCR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(twenty_news_train.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the Classes, we Roughly have a few main \"Categories\" The articles Cover, with numerous subtopics, (that are a large part but not completely encompassing the main categories)\n",
    "1. Science \n",
    "2. Religion \n",
    "3. Politics \n",
    "4. Recreation/Sports\n",
    "5. Technology/Computers\n",
    "\n",
    "In an ideal world, we would see the model. be able to seperate all these labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Step 1 Setting up Data Frames and Tokenizing \n",
    "base_data_train=twenty_news_train.data\n",
    "base_data_test=twenty_news_test.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing the data\n",
    "\n",
    "\n",
    "#Converting the Various Corpus to a list of sentences\n",
    "def article_to_sents(article):\n",
    "    nlp = spacy.load('en_core_web_sm') \n",
    "    sentences = [i.text for i in nlp(article).sents]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "#Takes a list of sentences and creates a list of the tokenized sentences \n",
    "#https://towardsdatascience.com/how-to-use-bert-from-the-hugging-face-transformer-library-d373a22b0209\n",
    "\n",
    "\n",
    "def sents_to_tokenized_list(marked_sents): \n",
    "    tokenizer= BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "    tokenized_list=[]\n",
    "    maskingattention=[]\n",
    "\n",
    "    #Normally, for Bert, We would tokenize it first and then take the tokenized response and encode it with the id's from the library in two seperate parts \n",
    "    #However, with encode_plus, both are done at once. Add Special tokens add the specific tokens needed that the bert model is trained for.\n",
    "    for sents in marked_sents: \n",
    "        tokens=tokenizer.encode_plus(sents, add_special_tokens = True,truncation = True,  max_length = 52, padding='max_length',return_attention_mask = True, return_tensors = \"pt\")\n",
    "        tokenized_list.append(tokens[\"input_ids\"])\n",
    "        maskingattention.append(tokens['attention_mask'])\n",
    "\n",
    "    return tokenized_list,maskingattention,\n",
    "\n",
    "\n",
    "\n",
    "def create_dataframe(base_data): \n",
    "\n",
    "    df= pd.DataFrame(base_data,columns=[\"Articles\"])\n",
    "    sentences=[] \n",
    "    tokens=[] \n",
    "    masking=[]\n",
    "  \n",
    "    for a in range(df.shape[0]):\n",
    "        article= df.iloc[a].values[0]\n",
    "        sents=article_to_sents(article)\n",
    "        sentences.append(sents)\n",
    "        tokenized_list, mask=sents_to_tokenized_list(sents)\n",
    "        tokens.append(tokenized_list) \n",
    "        masking.append(mask)\n",
    "      \n",
    "      \n",
    "    df[\"Sentences\"]=sentences \n",
    "    df[\"Tokens\"]=tokens \n",
    "    df[\"Attention_Masking\"]=masking\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=create_dataframe(base_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#due to the incredible length it takes to run a portion of code and dataframe's do not like tensors (as it saves it as strings, tocsv_converter and csv_converter aim to help save at least the raw data beore embeddings)\n",
    "def tocsv_converter(df):\n",
    "    \n",
    "    df=df[[\"Tokens\",\"Attention_Masking\"]]\n",
    "    temp_tokens=[] \n",
    "    temp_masking=[]\n",
    "\n",
    "    for a in range(df[\"Tokens\"].shape[0]): \n",
    "       \n",
    "        token=df[\"Tokens\"][a]\n",
    "        mask=df[\"Attention_Masking\"][a]\n",
    "        individual_token=[]\n",
    "        individual_mask=[]\n",
    "        \n",
    "        for b in range(len(token)): \n",
    "            \n",
    "            individual_token.append(token[b].tolist()) \n",
    "            individual_mask.append(mask[b].tolist())\n",
    "            \n",
    "        temp_tokens.append(individual_token)\n",
    "        temp_masking.append(individual_mask) \n",
    "\n",
    "    df[\"Tokens\"]=temp_tokens \n",
    "    df[\"Attention_Masking\"]=temp_masking\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_converter(df):\n",
    "\n",
    "\n",
    "    temp_tokens=[] \n",
    "    temp_masking=[]\n",
    "    df=df[[\"Tokens\",\"Attention_Masking\"]] \n",
    "    for a in range(len(df)): \n",
    "        token=ast.literal_eval(df[\"Tokens\"][a])\n",
    "        mask=ast.literal_eval(df[\"Attention_Masking\"][a])\n",
    "        individual_token=[]\n",
    "        individual_mask=[]\n",
    "        for b in range(len(token)): \n",
    "           individual_token.append(torch.tensor((token[b])))\n",
    "           individual_mask.append(torch.tensor((mask[b])))\n",
    "        temp_tokens.append(individual_token)\n",
    "        temp_masking.append(individual_mask)  \n",
    "\n",
    "    \n",
    "    df[\"Tokens\"]=temp_tokens \n",
    "    df[\"Attention_Masking\"]=temp_masking\n",
    "    return(df)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grabbing the results of the pre-proccesssing before the embeddings\n",
    "df1=pd.read_csv(\"df.csv\")\n",
    "df1=csv_converter(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: Creating Embeddings \n",
    "#https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ \n",
    "\n",
    "\n",
    "def create_embeddings_1_sentence(df):\n",
    "    \n",
    "    model= BertModel.from_pretrained('bert-base-cased', output_hidden_states = True) \n",
    "    model.eval() \n",
    "    num=(df[\"Tokens\"][0][0]).size(1) \n",
    "    \n",
    "    final_outputs=[]\n",
    "    for a in range(df.shape[0]):\n",
    "\n",
    "        tokens_list=df['Tokens'][a]\n",
    "        masking_list=df[\"Attention_Masking\"][a]\n",
    "        outputs_list=[]\n",
    "\n",
    "        for b in range(len(tokens_list)):\n",
    "            token=tokens_list[b]\n",
    "            masking=masking_list[b]\n",
    "            type(masking)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(token,attention_mask=masking) \n",
    "            outputs_list.append(outputs)\n",
    "        final_outputs.append(outputs_list) \n",
    "   \n",
    "    df[\"output_1_sentence\"]=final_outputs\n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=create_embeddings_1_sentence(df1[0:200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"outputs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_to_csv(df): \n",
    "    temp_output=[] \n",
    "\n",
    "    for a in range(df[\"output_1_sentence\"].shape[0]): \n",
    "       \n",
    "        article=df[\"output_1_sentence\"][a]\n",
    "        individual_output=[]\n",
    "\n",
    "        for b in range(len(article)): \n",
    "            \n",
    "            individual_token.append(article[b].tolist()) \n",
    "            \n",
    "        temp_output.append(individual_output)\n",
    "    \n",
    "\n",
    "    df[\"output_1_sentence\"]=temp_output\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def outputs_from_csv(df): \n",
    "    temp_output=[] \n",
    "\n",
    "    for a in range(df[\"output_1_sentence\"].shape[0]): \n",
    "       \n",
    "        article=ast.literal_eval(df[\"output_1_sentence\"][a])\n",
    "        individual_output=[]\n",
    "\n",
    "        for b in range(len(article)): \n",
    "            \n",
    "            individual_token.append(torch.tensor((article[b])))\n",
    "            \n",
    "        temp_output.append(individual_output)\n",
    "    \n",
    "\n",
    "    df[\"output_1_sentence\"]=temp_output\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now that we have the outputs,which are the values after each of the 12 hidden layers and the output we need to to extract the word/sentence embeddings\n",
    "#Lets start with sentence embeddings\n",
    "#inspired by https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ though not exactely the same.\n",
    "\n",
    "def bert_output_sentence_converter(output): \n",
    "    #output[2] is the hidden layer's final values + output of the model\n",
    "    #size= [13, 1, 52, 768]\n",
    "    hidden_layers_output=torch.stack(output[2], dim=0) \n",
    "   \n",
    "    # I borrowed my Extraction decision from the link below, to take the last four hidden layers and get the sum of them.\n",
    "    # check out http://jalammar.github.io/illustrated-bert/ for an intresting study on which hidden model we should extract from\n",
    "    final_four_layer_word_embeddings=hidden_layers_output[-4:][0] \n",
    "\n",
    "    sum_of_four_layers=torch.sum(torch.stack([final_four_layer_word_embeddings]), dim=0)\n",
    "    individual_word_embeddings = torch.squeeze(sum_of_four_layers, dim=0)\n",
    "\n",
    "    #to get a \"sentence embedding\" out of the word embeddings, I will simply get the average of all the word embeddings for each sentence\n",
    "    sentence_embeddings=torch.mean(individual_word_embeddings , dim=0)\n",
    "    \n",
    "    return sentence_embeddings\n",
    "\n",
    "\n",
    "bert_sentence_embeddings=[]\n",
    "for a in range(df.shape[0]): \n",
    "    article_output=df[\"output_1_sentence\"][a]\n",
    "    sentence_embeddings=[]\n",
    "\n",
    "    for b in range(len(article_output)):\n",
    "        sentence_output=article_output[b]\n",
    "        #takes sentence and calculated sentence embeddings\n",
    "        embedding=bert_output_sentence_converter(sentence_output)\n",
    "        sentence_embeddings.append(embedding)\n",
    "\n",
    "    bert_sentence_embeddings.append(sentence_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bert_sentence_embeddings)) \n",
    "print(bert_sentence_embeddings[0][1].size())\n",
    "#size of bert size embeddings (#number of article numbers, #number of sentences per ,# word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwf=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word Embeddings will be similar to Sentence embeddings\n",
    "\n",
    "#This is inspired by https://mccormickml.com/2019/05/14/BERT-word-embeddings-tutorial/ though how the word embedding itself is created is different\n",
    "def bert_output_word_converter(output): \n",
    "    #output[2] is the hidden layer's final values + output of the model\n",
    "    #size= [13, 1, 52, 768]\n",
    "    hidden_layers_output=torch.stack(output[2], dim=0) \n",
    "   \n",
    "    #normally, bert will have a batches (the [1] dimension), but since we have one, lets squeeze it out\n",
    "      #size= [13, 52, 768]\n",
    "    hidden_layers_output = torch.squeeze(hidden_layers_output, dim=1)\n",
    "    \n",
    "    # we now have a tensor with the right dimensions, but it would be easier if we could go word by word rather than layer by layer \n",
    "    #size[52,12,768]\n",
    "    hidden_layers_output = hidden_layers_output.permute(1,0,2)\n",
    "\n",
    "    \n",
    "    # I borrowed my Extraction decision from the link below, to take the last four hidden layers and get the sum of them \n",
    "    # check out http://jalammar.github.io/illustrated-bert/ for an intresting study on which hidden model we should extract from\n",
    "   \n",
    "\n",
    "    word_embeddings=[]\n",
    "    for word in hidden_layers_output:\n",
    "        \n",
    "        sum_of_hidden=torch.sum(torch.stack([word[-4],word[-3],word[-2],word[-1]]), dim=0)\n",
    "        word_embeddings.append(sum_of_hidden)\n",
    "\n",
    "    return word_embeddings\n",
    "\n",
    "\n",
    "bert_word_embeddings=[]\n",
    "for a in range(df.shape[0]): \n",
    "    article_output=df[\"output_1_sentence\"][a]\n",
    "    word_embeddings=[]\n",
    "\n",
    "    for b in range(len(article_output)):\n",
    "        sentence_output=article_output[b]\n",
    "        #takes a sentence and converts it into its word embeddings\n",
    "        embedding=bert_output_word_converter(sentence_output)\n",
    "        word_embeddings.append(embedding)\n",
    "        \n",
    "\n",
    "    bert_word_embeddings.append(word_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now that we have a tf_idf, matrix, we notice that there is a problem with sizing? For the sake of this project, for the tf_idf, we will \n",
    "# we will pool tf_idf function with a linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "#Transformer While we do have already established \"Groupings\"from the 20 Newsgroup such as on Cars or certain Sports, with the massive amount of data, \n",
    "# I want to look at if Sentemce Embeddings pulled from bert works significantly better than lets say a simple TF-IDF Vector as a representation of the data in grouping these articles without a \n",
    "#Label\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self,input_length=768,lstm_hidden_size=768,sequence_length=1,num_of_classes=20):\n",
    "        super().__init__()\n",
    "        ##YOUR CODE HERE##\n",
    "        \n",
    "        \n",
    "        self.input_length=input_length \n",
    "        self.lstm_hidden_size=lstm_hidden_size \n",
    "        self.sequence_length=sequence_length\n",
    "        self.num_of_classes=num_of_classes\n",
    "     \n",
    "\n",
    "        #Step 1 RNN/lstm\n",
    "\n",
    "        self.rnn= torch.nn.RNN(input_size=768,hidden_size=128,num_layers=1)\n",
    "        \n",
    "        #Step two Relu layers \n",
    "        self.relu_layer_1=torch.nn.Linear(lstm_hidden_size,128)\n",
    "        self.relu_layer_2=torch.nn.Linear(128,64)\n",
    "        \n",
    "        #Step 3 linear layer\n",
    "        self.linear_layer_1=torch.nn.Linear(64,num_of_classes)\n",
    "\n",
    "        #Step 4 Softmax layers \n",
    "        self.soft_max=torch.nn.Softmax(dim=2)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        h0 = torch.randn(1,1,self.lstm_hidden_size).requires_grad_()\n",
    "        \n",
    "        # Initialize cell state\n",
    "        \n",
    "        #Step 1 RNN/LSTM \n",
    "        x,hn =self.rnn(x,h0)\n",
    "      \n",
    "        if self.sequence_length>1: \n",
    "            x=torch.sum(x,dim=0)\n",
    "        \n",
    "        #Step two Relu dense layers\n",
    "        \n",
    "        y=self.relu_layer_1(x)\n",
    "        y=torch.nn.functional.relu(y) \n",
    "        y=self.relu_layer_2(y) \n",
    "        y=torch.nn.functional.relu(y)\n",
    "        #Step 3 linear layers \n",
    "        y=self.linear_layer_1(y) \n",
    "\n",
    "        #Step 4 Softmax layer\n",
    "        predictions=self.soft_max(y)\n",
    "      \n",
    "              \n",
    "        return(predictions) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the idea of my loss function is that  there needs to be away to \"reward\" the model if it predicts \n",
    "# a similar category (i.e if the class related to atheism was mistaken for an article on christianity, there should be some difference compared to if it was predicted as a basketball article)\n",
    "\n",
    "def individual_loss_function(predictions,reality):\n",
    "    #religion\n",
    "    key=[[0,15,18],[1,2,3,4,5],[6],[7,8,9,10],[11,12,13,14],[16,17,18]]\n",
    "    \n",
    "\n",
    "\n",
    "    predictions=predictions.squeeze(dim=0)\n",
    "    predictions=predictions.squeeze(dim=0)\n",
    "    #for loss one, i am only considering two things. If it is in the right group or not (a binary classifcation)\n",
    " \n",
    "    #checks if it got in the \"ballpark\" categories\n",
    "    pos_prob=0\n",
    "    for group in range(len(key)): \n",
    "        \n",
    "        if reality in key[group]:\n",
    "            for a in range(len(key[group])):\n",
    "        \n",
    "                pos_prob+=predictions[a]\n",
    "\n",
    "    pos_prob=torch.tensor(pos_prob)         \n",
    "    loss1=-1*torch.log(pos_prob)\n",
    "    #general cross loss function\n",
    "    loss2=-1*torch.log(predictions[reality])\n",
    "    loss=loss1+loss2\n",
    "    \n",
    "    return loss\n",
    "    \n",
    "def run_model_sentence(bert_sentence_embeddings,epoch=100):\n",
    "    \n",
    "    learning_rate=.05\n",
    "    epoch=epoch\n",
    "    \n",
    "    article_realities=twenty_news_train.target[0:len(bert_sentence_embeddings)]\n",
    "  \n",
    "    \n",
    "    sentence_model=Model(input_length=768,lstm_hidden_size=128)\n",
    "    optimizer = torch.optim.SGD(sentence_model.parameters(), lr=learning_rate) \n",
    "    final_predictions=[]\n",
    "    for epochs in range(epoch):\n",
    "        for a in range(len(bert_sentence_embeddings)): \n",
    "            \n",
    "            article=bert_sentence_embeddings[a]\n",
    "            article_reality=article_realities[a]\n",
    "            prediction_list=[] \n",
    "            for b in range(len(article)):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                x=article[b]\n",
    "                x=x.unsqueeze(dim=0)\n",
    "                x=x.unsqueeze(dim=0)\n",
    "               \n",
    "                predictions=sentence_model(x)\n",
    "               \n",
    "                loss= individual_loss_function(predictions,article_reality)\n",
    "\n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "                \n",
    "                if epochs==(epoch-1):\n",
    "                    prediction=predictions.squeeze(dim=0) \n",
    "                    prediction=prediction.squeeze(dim=0) \n",
    "                    prediction=torch.argmax(prediction)\n",
    "                    sent_prediction=predictions.argmax()\n",
    "                    prediction_list.append(sent_prediction.item())\n",
    "                  \n",
    "            \n",
    "            if epochs==(epoch-1):\n",
    "                article_prediction=max(set(prediction_list), key=prediction_list.count)\n",
    "                final_predictions.append(article_prediction)\n",
    "                \n",
    "        if epochs%20==0: \n",
    "            print(loss)\n",
    "       \n",
    "    \n",
    "    return(sentence_model,final_predictions)\n",
    "        \n",
    "        \n",
    "        \n",
    "def run_model_word(bert_word_embeddings,epoch=100): \n",
    "\n",
    "    \n",
    "    learning_rate=.05\n",
    "    epoch=epoch\n",
    "    \n",
    "    article_realities=twenty_news_train.target[0:len(bert_word_embeddings)]\n",
    "    word_model=Model(input_length=768,lstm_hidden_size=128)\n",
    "    optimizer = torch.optim.SGD(word_model.parameters(), lr=learning_rate) \n",
    "    final_predictions=[]\n",
    "    for epochs in range(epoch):\n",
    "        \n",
    "        for a in range(len(bert_word_embeddings)): \n",
    "            \n",
    "            article=bert_word_embeddings[a]\n",
    "            article_reality=article_realities[a]\n",
    "            prediction_list=[] \n",
    "            for b in range(len(article)):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                #sentence is a list of tensors (52,768)\n",
    "                sentence=article[b]\n",
    "                #lets convert to a tensor of lists\n",
    "                #(52,768)\n",
    "                sentence=torch.stack(sentence[:])\n",
    "        \n",
    "\n",
    "                #x is a (1,52,768)\n",
    "                x=sentence.unsqueeze(dim=0)\n",
    "                #x= (52,1,769)\n",
    "                x=x.permute(1,0,2)\n",
    "             \n",
    "               \n",
    "                predictions=word_model(x)\n",
    "                \n",
    "                \n",
    "                loss= individual_loss_function(predictions,article_reality) \n",
    "                loss=loss[0].sum()/20\n",
    "    \n",
    "                loss.backward() \n",
    "                optimizer.step()\n",
    "                \n",
    "                if epochs==(epoch-1):\n",
    "                    prediction=predictions[51]\n",
    "                    prediction=prediction.squeeze(dim=0) \n",
    "                    word_prediction=torch.argmax(prediction) \n",
    "                    prediction_list.append(word_prediction.item())\n",
    "\n",
    "\n",
    "            if epochs==(epoch-1):\n",
    "                \n",
    "                article_prediction=max(set(prediction_list), key=prediction_list.count)\n",
    "                final_predictions.append(article_prediction) \n",
    "        \n",
    "        if epochs%20==0: \n",
    "            print(loss)  \n",
    "    \n",
    "            \n",
    "            \n",
    "                \n",
    "    \n",
    "    \n",
    "    return(word_model,final_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4\n",
    "#Now how do we determine success? lets look at some insights\n",
    "\n",
    "def precise_confusion_matrix(actual,prediction_orig,length):\n",
    "    \n",
    "    list_of_predictions=[]\n",
    "    for a in range(length):\n",
    "        prediction=prediction_orig[a]\n",
    "        list_of_predictions.append(prediction) \n",
    "       \n",
    "    #grabbed from https://stackoverflow.com/questions/38877301/how-to-calculate-accuracy-based-on-two-lists-python\n",
    "    list_of_predictions=np.array(list_of_predictions)\n",
    "    actual=np.array(actual)\n",
    "    correct = (list_of_predictions == actual)\n",
    "    accuracy = float(correct.sum()/length)\n",
    "    print(accuracy)\n",
    "    \n",
    "    \n",
    "    \n",
    "def general_confusion_matrix(actual,prediction_orig,length): \n",
    "    key=[[0,15,18],[1,2,3,4,5],[6],[7,8,9,10],[11,12,13,14],[16,17,18]]\n",
    "    \n",
    "    list_of_predictions=[]\n",
    "    for a in range(length):\n",
    "        prediction=prediction_orig[a]\n",
    "        list_of_predictions.append(prediction)\n",
    "    \n",
    "    for b in range(length): \n",
    "        for group in range(len(key)): \n",
    "            if list_of_predictions[b] in key[group]: \n",
    "                list_of_predictions[b]=group\n",
    "            if actual[b] in key[group]: \n",
    "                actual[b]=group      \n",
    "   \n",
    "\n",
    "    list_of_predictions=np.array(list_of_predictions)\n",
    "    actual=np.array(actual)\n",
    "    correct = (list_of_predictions == actual)  \n",
    "    accuracy=float(correct.sum()/length)\n",
    "    print(accuracy)\n",
    "\n",
    "    \n",
    "     \n",
    "    \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "4670fc71c1cae8bb0a1a459834888818b1db862995a203c305d8455da6d800ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
